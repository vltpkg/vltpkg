name: Run Benchmarks

on:
  pull_request:
    types: [opened, reopened, edited, synchronize]

jobs:
  benchmark_fork_pr_branch:
    name: Run Fork PR Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      matrix:
        fixture: ['vue']
        variation:
          [
            'cache-lockfile',
            'cache-lockfile-node-modules',
            'clean',
            'lockfile',
          ]
    env:
      BENCH_WARMUP: '2'
      BENCH_RUNS: '10'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Nodejs
        uses: actions/setup-node@v4
        with:
          node-version: '^22.22.0'
          check-latest: true

      - name: Install Dependencies with known good version of vlt
        run: npx vlt@latest install

      - name: Build source version of vlt
        run: node --run build:bundle

      - name: Add source version of vlt to PATH
        run: echo "$GITHUB_WORKSPACE/scripts/bins/bundle" >> $GITHUB_PATH

      - name: Install Dependencies with source of vlt
        run: |
          # avoid changes from the bootstrap step
          ./scripts/rm-node-nodules.ts
          # re-install dependencies
          vlt install --view=human

      - name: Prepare
        run: vlr --recursive prepare

      - name: Install & Setup Tools
        run: |
          bash ./infra/cli-benchmarks/scripts/setup.sh

      - name: Run Benchmarks variations
        run: |
          bash ./infra/cli-benchmarks/scripts/benchmark.sh ${{ matrix.fixture }} ${{ matrix.variation }}

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.fixture }}-${{ matrix.variation }}
          path: ./results/${{ matrix.fixture }}/${{ matrix.variation }}/

  consolidate:
    name: Consolidate Benchmark Results
    needs: [benchmark_fork_pr_branch]
    runs-on: ubuntu-latest
    steps:
      - name: Download Results
        uses: actions/download-artifact@v4
        with:
          path: results
          pattern: results-*

      - name: Clean benchmarks result
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Define fixture and variation values from the strategy matrix
            const fixtures = ["vue"];
            const variations = ["cache-lockfile", "cache-lockfile-node-modules", "clean", "lockfile"];

            // Helper functions for statistical calculations
            function calculateMean(times) {
              return times.reduce((sum, time) => sum + time, 0) / times.length;
            }

            function calculateStddev(times, mean) {
              const variance = times.reduce((sum, time) => sum + Math.pow(time - mean, 2), 0) / times.length;
              return Math.sqrt(variance);
            }

            function calculateMedian(times) {
              const sorted = [...times].sort((a, b) => a - b);
              const mid = Math.floor(sorted.length / 2);
              return sorted.length % 2 === 0
                ? (sorted[mid - 1] + sorted[mid]) / 2
                : sorted[mid];
            }

            // Clean benchmark results
            for (const fixture of fixtures) {
              for (const variation of variations) {
                const benchmarkPath = path.join('results', `results-${fixture}-${variation}`, 'benchmarks.json');

                try {
                  console.log(`Cleaning benchmark file: ${benchmarkPath}`);
                  const benchmarkData = JSON.parse(fs.readFileSync(benchmarkPath, 'utf8'));

                  if (benchmarkData.results && benchmarkData.results.length > 0) {
                    const result = benchmarkData.results[0];
                    const { times, exit_codes } = result;

                    if (times && exit_codes && times.length === exit_codes.length) {
                      // Filter out times where exit_codes is not 0
                      const cleanTimes = times.filter((time, index) => exit_codes[index] === 0);
                      const cleanExitCodes = exit_codes.filter(code => code === 0);

                      if (cleanTimes.length > 0) {
                        // Recalculate statistics
                        const mean = calculateMean(cleanTimes);
                        const stddev = calculateStddev(cleanTimes, mean);
                        const median = calculateMedian(cleanTimes);
                        const min = Math.min(...cleanTimes);
                        const max = Math.max(...cleanTimes);

                        // Update the result object
                        result.times = cleanTimes;
                        result.exit_codes = cleanExitCodes;
                        result.mean = mean;
                        result.stddev = stddev;
                        result.median = median;
                        result.min = min;
                        result.max = max;

                        console.log(`Cleaned ${fixture}-${variation}: ${times.length - cleanTimes.length} failed runs removed, ${cleanTimes.length} valid runs remaining`);
                      } else {
                        console.warn(`All runs failed for ${fixture}-${variation}`);
                      }
                    } else {
                      console.warn(`Invalid times/exit_codes arrays for ${fixture}-${variation}`);
                    }

                    // Save the cleaned data back to the file
                    fs.writeFileSync(benchmarkPath, JSON.stringify(benchmarkData, null, 2));
                  } else {
                    console.warn(`No results found in ${benchmarkPath}`);
                  }
                } catch (error) {
                  console.error(`Failed to clean ${benchmarkPath}: ${error.message}`);
                }
              }
            }

            console.log('Benchmark cleaning completed');

      - name: Consolidate benchmarks
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Define fixture and variation values from the strategy matrix
            const fixtures = ["vue"];
            const variations = ["cache-lockfile", "cache-lockfile-node-modules", "clean", "lockfile"];

            const consolidatedResults = {
              results: []
            };

            // Read and consolidate benchmark results
            for (const fixture of fixtures) {
              for (const variation of variations) {
                const benchmarkPath = path.join('results', `results-${fixture}-${variation}`, 'benchmarks.json');

                try {
                  console.log(`Reading benchmark file: ${benchmarkPath}`);
                  const benchmarkData = JSON.parse(fs.readFileSync(benchmarkPath, 'utf8'));

                  // Take the first entry from the results array
                  if (benchmarkData.results && benchmarkData.results.length > 0) {
                    consolidatedResults.results.push(benchmarkData.results[0]);
                    console.log(`Added result for ${fixture}-${variation}: ${benchmarkData.results[0].command}`);
                  } else {
                    console.warn(`No results found in ${benchmarkPath}`);
                  }
                } catch (error) {
                  console.error(`Failed to read ${benchmarkPath}: ${error.message}`);
                }
              }
            }

            // Save consolidated results
            const outputPath = 'benchmarks.json';
            fs.writeFileSync(outputPath, JSON.stringify(consolidatedResults, null, 2));
            console.log(`Consolidated ${consolidatedResults.results.length} benchmark results to ${outputPath}`);

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmarks.json
          path: benchmarks.json

      - name: Upload GitHub Pull Request Event
        uses: actions/upload-artifact@v4
        with:
          name: event.json
          path: ${{ github.event_path }}
